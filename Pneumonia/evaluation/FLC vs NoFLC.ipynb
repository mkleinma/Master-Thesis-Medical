{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873e1dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:562\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    564\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "import pydicom \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "\n",
    "from libraries import augmentations\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Paths\n",
    "csv_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "\n",
    "#csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "model_path_flc = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_FLC\\light_oversamp\\seed_1\\pneumonia_detection_model_resnet_bestf1_1.pth\"\n",
    "model_path_normal = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos\\light_oversamp_nonorm\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1.pth\"\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_worse = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model_worse.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model_worse.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model_worse.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model_worse.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model_worse.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model_worse.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)    \n",
    "model_worse.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "state_dict_worse = torch.load(model_path_flc, map_location=device)\n",
    "\n",
    "model_worse.load_state_dict(state_dict_worse)\n",
    "model_worse = model_worse.to(device)\n",
    "model_worse.eval()\n",
    "\n",
    "# Load model\n",
    "model_better = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model_better.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "state_dict_better = torch.load(model_path_normal, map_location=device)\n",
    "\n",
    "model_better.load_state_dict(state_dict_better)\n",
    "model_better = model_better.to(device)\n",
    "model_better.eval()\n",
    "\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "    ])\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "first_split = splits[0]\n",
    "val_idx = first_split[1]  # Only use the validation indices from the 5th split\n",
    "val_data = data.iloc[val_idx]\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "directory = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\comparison_images\\FLC_Diff_Test\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        six_channel_images = []\n",
    "        if i > 5:\n",
    "            break\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model_worse.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        outputs = model_worse(six_channel_images)  # Logits\n",
    "        probs = torch.softmax(outputs, dim=1)  # Probabilities\n",
    "        preds = torch.argmax(probs, dim=1)  # Binary predictions\n",
    "        for image, patient_id in zip(six_channel_images, patient_ids):\n",
    "            i += 1\n",
    "            if i > 50:\n",
    "              break\n",
    "            image = image[None]\n",
    "            expl_worse = model_worse.explain(image)\n",
    "            filename = f\"{patient_id}_worse_explanation.png\"\n",
    "            image_path_worse = os.path.join(directory, filename)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(expl_worse[\"explanation\"])\n",
    "            plt.axis('off')\n",
    "            plt.savefig(image_path_worse, bbox_inches=\"tight\", pad_inches=0)\n",
    "            plt.close()\n",
    "        \n",
    "        \n",
    "            expl_better = model_better.explain(image)\n",
    "            filename = f\"{patient_id}_better_explanation.png\"\n",
    "            image_path = os.path.join(directory, filename)\n",
    "                    \n",
    "            plt.figure()\n",
    "            plt.imshow(expl_better[\"explanation\"])\n",
    "            plt.axis('off')\n",
    "            plt.savefig(image_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "            plt.close()\n",
    "\n",
    "            subtracted_expl = expl_better[\"contribution_map\"] - expl_worse[\"contribution_map\"]\n",
    "            print(subtracted_expl)\n",
    "          \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(subtracted_expl, cmap='coolwarm')\n",
    "            plt.colorbar(label=\"Contribution\")\n",
    "            plt.title(\"Contribution Map\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            image_path = os.path.join(directory, f\"{patient_id}_contributionmap.png\")\n",
    "            plt.savefig(image_path)\n",
    "            print(f\"Contribution map saved to {patient_id}_contributionmap.png\")\n",
    "            plt.show()\n",
    "\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45cbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
