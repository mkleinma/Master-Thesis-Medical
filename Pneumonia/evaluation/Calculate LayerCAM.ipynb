{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerCAM Calculation for Baseline Models\n",
    "use the second cell for B-Cos networks because they require different preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average LayerCAM Energy-Based Pointing Game Proportion: 0.1736\n",
      "Average LayerCAM Energy-Based Pointing Game Proportion of Incorrectly Classified Images: 0.1157, Count: 297\n",
      "Average LayerCAM Energy-Based Pointing Game Proportion of Correctly Classified Images: 0.1926, Count: 905\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "import pydicom \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "\n",
    "from cam.layercam import LayerCAM\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.energyPointGame import energy_point_game\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_Baseline\\oversampling_light\\seed_0\\pneumonia_detection_model_resnet_baseline_bestf1_1.pth\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet_BCos\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1_26.pth\"\n",
    "\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universit채t\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universit채t\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "#model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "    \n",
    "\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "model_dict = dict(\n",
    "    type=\"resnet_base\",\n",
    "    layer_name=\"layer4\",\n",
    "    arch=model,\n",
    "    target_layer=model.layer4[-1].conv2 # Example: last layer of ResNet's layer4  ###### IN BCOS:     target_layer=model.layer4[-1].conv3  # Example: last layer of ResNet's layer4  ### double check!\n",
    "\n",
    ")\n",
    "\n",
    "cam = LayerCAM(model_dict)\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "for images, labels, patient_ids in val_loader:\n",
    "    with torch.set_grad_enabled(True):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #six_channel_images = []\n",
    "        #for img_tensor in images:\n",
    "        #    numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        #    pil_image = Image.fromarray(numpy_image)\n",
    "        #    transformed_image = model.transform(pil_image)\n",
    "        #    six_channel_images.append(transformed_image)\n",
    "        #six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        for image, label, patient_id in zip(images, labels, patient_ids): #zip(images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty:   \n",
    "                #image = image[None]\n",
    "                image = image.unsqueeze(0)\n",
    "                output = model(image) \n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "                contribution_map = cam(image).cpu()\n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    #x, y, width, height = row[\"x\"], row[\"y\"], row[\"width\"], row[\"height\"]\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    #print(contribution_map)\n",
    "                    contribution_map_2d = contribution_map.squeeze(0).squeeze(0)\n",
    "                    #plt.imshow(contribution_map.squeeze(0).squeeze(0).cpu().numpy(), cmap='jet', alpha=0.5)\n",
    "                    #plt.colorbar()\n",
    "                    #plt.title(f\"LayerCAM Heatmap for Patient {patient_id}\")\n",
    "                    #plt.show()\n",
    "                    proportion += energy_point_game(coordinates_tensor, contribution_map_2d)\n",
    "                        \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                    #print(\"Proportion Correct \" + str(proportion.item()))\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    #print(\"Proportion: Incorrect\" + str(proportion.item()))\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average LayerCAM Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average LayerCAM Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average LayerCAM Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerCAM for B-Cos Models\n",
    "execute the line below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average LayerCAM Energy-Based Pointing Game Proportion: 0.1957\n",
      "Average LayerCAM Energy-Based Pointing Game Proportion of Incorrectly Classified Images: 0.1049, Count: 260\n",
      "Average LayerCAM Energy-Based Pointing Game Proportion of Correctly Classified Images: 0.2208, Count: 942\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "import pydicom \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "\n",
    "from cam.layercam import LayerCAM\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.energyPointGame import energy_point_game\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos\\light_oversamp\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1.pth\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet_BCos\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1_26.pth\"\n",
    "\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universit채t\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universit채t\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "    \n",
    "\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "model_dict = dict(\n",
    "    type=\"resnet_bcos\",\n",
    "    layer_name=\"layer4\",\n",
    "    arch=model,\n",
    "    target_layer=model.layer4[-1].conv2 # Example: last layer of ResNet's layer4  ###### IN BCOS:     target_layer=model.layer4[-1].conv3  # Example: last layer of ResNet's layer4  ### double check!\n",
    "\n",
    ")\n",
    "\n",
    "cam = LayerCAM(model_dict)\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "for images, labels, patient_ids in val_loader:\n",
    "    with torch.set_grad_enabled(True):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids): #zip(images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty:   \n",
    "                image = image.unsqueeze(0)\n",
    "                output = model(image) \n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "                contribution_map = cam(image).cpu()\n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    #x, y, width, height = row[\"x\"], row[\"y\"], row[\"width\"], row[\"height\"]\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    #print(contribution_map)\n",
    "                    contribution_map_2d = contribution_map.squeeze(0).squeeze(0)\n",
    "                    #plt.imshow(contribution_map.squeeze(0).squeeze(0).cpu().numpy(), cmap='jet', alpha=0.5)\n",
    "                    #plt.colorbar()\n",
    "                    #plt.title(f\"LayerCAM Heatmap for Patient {patient_id}\")\n",
    "                    #plt.show()\n",
    "                    proportion += energy_point_game(coordinates_tensor, contribution_map_2d)\n",
    "                        \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                    #print(\"Proportion Correct \" + str(proportion.item()))\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    #print(\"Proportion: Incorrect\" + str(proportion.item()))\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average LayerCAM Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average LayerCAM Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average LayerCAM Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
