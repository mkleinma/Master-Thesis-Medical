{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-based Pointing Game\n",
    "\n",
    "Cell directly below:\n",
    "- iterates one model (e.g. best performing fold 1 model of B-Cos ResNet50)\n",
    "- determines Energy-based pointing game of the fold fitting to the model (validation set) - first fold model focuses on validation set of first fold\n",
    "- calculates average energy-based pointing game result of the validation set\n",
    "- Results: higher proportion (result of energy-based pointing game) on correctly classified images\n",
    "\n",
    "Second Cell below:\n",
    "- prints image with corresponding explanation image\n",
    "- executes Energy-based pointing game on one image - giving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.3479\n",
      "Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: 0.1664, Count: 965\n",
      "Average Energy-Based Pointing Game Proportion of Correctly Classified Images: 0.533, Count: 946\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos_FLC\\seed_0_new\\pneumonia_detection_model_bcos_flc_bestf1_1_14.pth\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\UniversitÃ¤t\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Modify model and load model\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    proportion += energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image and execute Energy-based Pointing Game for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-cos/B-cos-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \n\u001b[0;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m NormedConv2d(\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m2\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# code from B-cos paper reused to adjust network\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Load trained weights (optional)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1451\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:202\u001b[0m, in \u001b[0;36m_rebuild_tensor_v2\u001b[1;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor_v2\u001b[39m(\n\u001b[0;32m    200\u001b[0m     storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    201\u001b[0m ):\n\u001b[1;32m--> 202\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_rebuild_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m requires_grad\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:180\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[1;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mset_(storage\u001b[38;5;241m.\u001b[39m_untyped_storage, storage_offset, size, stride)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "from sklearn.model_selection import KFold\n",
    "from bcosconv2d import NormedConv2d\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from libraries.energyPointGame import energy_point_game\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\BestPerformingResNet50BCosResult\\pneumonia_detection_model_bcos_trans_bestf1_1_24.pth\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "\n",
    "\n",
    "torch.hub.list('B-cos/B-cos-v2')\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)    \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Load trained weights (optional)\n",
    "model = model.to(device)\n",
    "\n",
    "patient_id = \"073630e6-3b52-4b41-a14b-bf8bb57e28ea\"\n",
    "image_path = os.path.join(image_folder,\"073630e6-3b52-4b41-a14b-bf8bb57e28ea.dcm\")\n",
    "dicom = pydicom.dcmread(image_path)\n",
    "image = Image.fromarray(dicom.pixel_array).convert(\"RGB\")\n",
    "\n",
    "img_new = model.transform(image)\n",
    "img_new = img_new[None]\n",
    "img_new = img_new.to(device)\n",
    "\n",
    "\n",
    "# A: 19.297964096069336, B: -0.012265102937817574 sum positive and negative values\n",
    "\n",
    "model.eval()\n",
    "expl = model.explain(img_new)\n",
    "explanation_image_array = expl['explanation']\n",
    "explanation_image_array = (explanation_image_array * 255).astype(np.uint8)\n",
    "explanation_image = Image.fromarray(explanation_image_array)\n",
    "\n",
    "\n",
    "contribution_map = expl['contribution_map'].squeeze(0)\n",
    "#positive_map = torch.where(contribution_map>=0, 0, contribution_map)\n",
    "#contribution_map_no_neg = torch.where(contribution_map<0, 0, contribution_map)\n",
    "\n",
    "# Plot side by side\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# DICOM image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(f\"DICOM Image - {patient_id}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(explanation_image)\n",
    "plt.title(f\"Explanation Image - {patient_id}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "my_image = r\"C:\\Users\\Admin\\Downloads\\prediction_comparison\\ff5efd4a-0777-41b9-82c7-b73b4073c509_normal_explanation.png\"\n",
    "original_image_name = find_original_image_name(image_folder, my_image)\n",
    "\n",
    "#print(contribution_map)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "filtered_rows = df[(df['patientId'] == '073630e6-3b52-4b41-a14b-bf8bb57e28ea') & (df['Target'] == 1)]\n",
    "\n",
    "for _, row in filtered_rows.iterrows():\n",
    "    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y) # not ints\n",
    "    \n",
    "    explanation_draw = ImageDraw.Draw(explanation_image)\n",
    "    explanation_draw.rectangle([(x, y), (x + width, y + height)], outline=\"red\", width=2)\n",
    "\n",
    "    coordinates_list = [x, y, x + width, y + height]\n",
    "    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "    contribution_map = contribution_map.cpu()\n",
    "    proportion = energy_point_game(coordinates_tensor, contribution_map)\n",
    "    \n",
    "    print(f\"Proportion {proportion}\")\n",
    "\n",
    "plt.imshow(explanation_image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## bbox and saliency map\n",
    "#energy_point_game(\"\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
